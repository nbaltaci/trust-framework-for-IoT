{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues with Previously Proposed Normalization Methods\n",
    "\n",
    "---\n",
    "\n",
    "There are four reasons for us to propose our own normalization methods. First, we may have only one IoT device collecting data items for an $mt$, so the $D$ constructed for $mt$ includes data items from a single IoT device. In this case, $D$ may include <i>identical values</i> for some of the attributes. For example, distance, key freshness or link type will most probably not change for a \"thing\" in a short time interval for which a $D$ is constructed. This case of identical attribute values leads to <i>zero denominator</i> for the following linear normalization formula presented in [[1]](#1) [[2]](#2):\n",
    "\n",
    "\\begin{equation}\n",
    "r_{ij}=\n",
    "    \\begin{cases}\n",
    "     \\frac{x_{ij}-{\\tilde{x}_j}}{{x^*_j}-{\\tilde{x}_j}}, & \\text{for benefit attributes} \\\\\n",
    "     \\frac{{x^*_j}-x_{ij}}{{x^*_j}-{\\tilde{x}_j}}, & \\text{for cost attributes}\n",
    "    \\end{cases}\n",
    "\\label{eq:zero_denominator}\n",
    "\\end{equation}\n",
    "\n",
    "where $x_{ij}$, $\\tilde{x}_j$, and $x^*_j$ denote respectively the original attribute value, minimum and maximum value of the $j$th attribute.\n",
    "\n",
    "Second, an identical attribute value leads to the problem of <i>zero normalized value</i> for the following linear normalization method presented in [[1]](#1) [[2]](#2), for cost attributes:\n",
    "\n",
    "\\begin{equation}\n",
    "r_{ij}=\n",
    "\\begin{cases}\n",
    "    \\frac{x_{ij}}{x^*_j}, & \\text{for benefit attributes} \\\\\n",
    "    1-\\frac{x_{ij}}{x^*_j}, & \\text{for cost attributes}\n",
    "\\end{cases}\n",
    "\\label{eq:zero_normalized_value}\n",
    "\\end{equation}\n",
    "\n",
    "If the normalized value of an attribute is zero, this causes discarding the attribute from the analysis. This is not a desirable scenario as the attribute itself (any of our attributes) is a distinguishing factor for comparing the trust of different measurements. For example, we may have just one water level sensor to measure the water level and three humidity sensors to measure the humidity. In this case, if we want to compare the trust of water level and humidity measurements, we cannot simply ignore an attribute with identical values due to zero normalized value. The attribute with identical values for water level measurement would not have identical values for humidity measurement.\n",
    "\n",
    "Third, an identical attribute value leads to the problem of <i>identical normalized value</i> for the following linear normalization method presented in [[1]](#1) [[2]](#2):\n",
    "\n",
    "\\begin{equation}\n",
    "r_{ij}=\\frac{x_{ij}}{\\sum\\limits_{i=1}^{m} x_{ij}}\n",
    "\\label{eq:identical_normalized_value}\n",
    "\\end{equation}\n",
    "\n",
    "The reason for identical normalized value to be a problem is that regardless of the actual value $x_{ij}$, the normalized value $r_{ij}$ will always be the same if the number of data items $d^{ID}_t$ is equal for two different $D$s. For instance, we may have one sensor for water level measurement and one sensor for humidity measurement. If $D$ for both $mt$ consists of the same number of $d^{ID}_t$s with identical values, then resulting $r_{ij}$ values will be the same. For example, assume that the temperature attribute has a fixed value in both matrices, say 100 $^{\\circ}$F for one sensor and 75 $^{\\circ}$F for another. Also assume that there are five $d^{ID}_t$s collected and five rows in both $D$s. Then, for both sensors, each $r_{ij}$ will be 0.2. This result does not reflect that the original temperature sensed by the two sensors are different and treat them equally in DM process.\n",
    "\n",
    "% Fourth, identical attribute values lead to <i>zero variance</i> and division by zero problem for the following non-monotonic normalization formula presented in [[1]](#1) [[2]](#2):\n",
    "  \n",
    "% \\begin{equation}\n",
    "% r_{ij}=e^{-\\frac{z^2}{2}},  z=\\frac{(x{ij}-x_j^0)}{\\sigma_j}\n",
    "% \\label{eq:zero_variance}\n",
    "% \\end{equation}\n",
    "% where $x_j^0$ is the most favorable value of attribute $j$ and $\\sigma_j$ is the standard deviation of the attribute values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In the paper, we present a method to convert trust scores into a <b>distrust interval</b>. We also experimented another method that is a \"conventional confidence interval method\" to obtain a <b>trust interval</b>. We eventually selected the method that we present in the paper as it yields a narrower interval and less uncertainty of opinion triplets. By conventional confidence interval method, we imply using t-distribution to compute a confidence interval out of trust scores. The use of t-distribution is suggested when there is no knowledge about the population's behavior or sample size is small \\cite{field2013discovering}. We represent a trust interval as $TI_{ID}=[LTB, UTB]$, where $LTB$ and $UTB$ mean lower trust bound and upper trust bound, respectively.  We compute the $TI_{ID}$ as follows for the first method:\n",
    "\n",
    "\\begin{equation*}\n",
    "LTB=max\\lbrace 0,\\overline{RFT(d^{ID}_{t_i})}-(t_{{n-1},{1-\\alpha/2}}\\times SE_{RFT(d^{ID}_{t_i})})\\rbrace\n",
    "\\end{equation*}\n",
    "\n",
    "\\begin{equation*}\n",
    "UTB=min\\lbrace 1,\\overline{RFT(d^{ID}_{t_i})}+(t_{{n-1},{1-\\alpha/2}}\\times SE_{RFT(d^{ID}_{t_i})})\\rbrace\n",
    "\\end{equation*}\n",
    "\n",
    "where $\\overline{RFT(d^{ID}_{t_i})}$ is average of scores, $n$ is the number of data items for a given device $ID$, $\\alpha=0.05$ (significance level), and $SE_{RFT(d^{ID}_{t_i})}$ is standard error of scores. \n",
    "\n",
    "Using $FT$ scores in Table below (Table IV in the paper) and equations above, we compute $TI_{ID}=[0.0673,1.0000]$. The method we used in our paper resulted in a distrust interval $DI_{ID}=[0.3803,0.5489]$. As seen from these, conventional confidence interval method yields a wider range for trust scores, which means that there are more possible values that the real trust value will be drawn and so it leads to a higher uncertainty.\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\\begin{array}{c|cc|cc|cc|c}\\hline\n",
    "Alternative & Key freshness &  & Data freshness & &Temperature & &FT   \\\\\n",
    "            & O & N         & O & N          & O & N       &      \\\\\\hline\n",
    "ID=1 d_1                    & 0.5000 & 0.5000                   & 0.0057 & 0.0057                      & 75 & 0.9460                     & 0.4122                                        \\\\\n",
    "ID=1 d_2                    & 0.5000 & 0.5000                   & 0.0059 & 0.0059                      & 75 & 0.9460                     & 0.4123                                        \\\\\n",
    "ID=1 d_3                    & 0.5000 & 0.5000                   & 0.0061 & 0.0061                      & 75 & 0.9460                     & 0.4123                                        \\\\\n",
    "ID=1 d_4                    & 1.0000 & 1.0000                   & 0.0063 & 0.0063                      & 74 & 0.9651                     & 0.7201                                        \\\\\n",
    "ID=1 d_5                    & 1.0000 & 1.0000                   & 0.0065 & 0.0065                      & 74 & 0.9651                     & 0.7201                                        \\\\\\hline\n",
    "Glob\\_min  &   &0   &  & 0  &   &0      \\\\\n",
    "Glob\\_max  &   & 1  &  & 1  &   &100    \\\\\n",
    "LPV_j    &   & -  &  & -  &   &32     \\\\\n",
    "UPV_j    &   & -  &  & -  &   &70     \\\\\\hline\n",
    "\\end{array}\n",
    "\n",
    "\n",
    "O: original value ($x_{ij}$), N: normalized value ($r_{ij}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## References\n",
    "<a id=\"1\">[1]</a> \n",
    "Hwang, C. L., & Yoon, K. (1981). Methods for multiple attribute decision making. In Multiple attribute decision making (pp. 58-191). Springer, Berlin, Heidelberg.\n",
    "\n",
    "<a id=\"2\">[2]</a> \n",
    "Yoon, K. P., & Hwang, C. L. (1995). Multiple attribute decision making: an introduction (Vol. 104). Sage publications.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
